{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "import pandas as pd\n",
    "import json\n",
    "import typing\n",
    "import requests\n",
    "\n",
    "# initial data import and processing\n",
    "# source_path needs to be .bib\n",
    "# store_path needs to be .csv\n",
    "def make_dataset(source_path,store_path):\n",
    "    \n",
    "    # import data as pandas dataframe\n",
    "    with open(source_path) as bibtex_file:\n",
    "        bib_database = bibtexparser.load(bibtex_file)\n",
    "        records_df = pd.DataFrame(bib_database.entries)\n",
    "        \n",
    "    # drop rows without target label\n",
    "    records_df.dropna(subset=\"literature_review\", inplace=True)\n",
    "    \n",
    "    # drop rows not published in basket of eight\n",
    "    top_8 = [\"European Journal of Information Systems\", \"Information Systems Journal\", \"Information Systems Research\", \"Journal of AIS\", \"Journal of Information Technology\", \"Journal of MIS\", \"Journal of Strategic Information Systems\", \"MIS Quarterly\"]\n",
    "    df = records_df.loc[records_df['journal'].isin(top_8)]\n",
    "\n",
    "    # drop specific rows with insufficient information\n",
    "    df = df[df.prescreen_exclusion != \"complementary material\"]\n",
    "    df = df[df.title != \"Editorial\"]\n",
    "    \n",
    "    # drop columns not needed for the features\n",
    "    column_filter = df.filter(['colrev.dblp.dblp_key', 'colrev_pdf_id', 'colrev_data_provenance', 'colrev_masterdata_provenance', 'colrev_status', 'colrev_origin', 'colrev.semantic_scholar.id', \n",
    "             'pdf_processed', 'prescreen_exclusion', 'note', 'fulltext', 'link', 'file', 'cited_by', 'screening_criteria', 'ID', 'crossmark-restriction', 'man_prep_hints', \n",
    "             'keywords', 'language', 'url', 'pages', 'number', 'volume', 'year', 'journal', 'author', 'ENTRYTYPE'])\n",
    "    df.drop(column_filter, axis=1, inplace=True)\n",
    "    \n",
    "    # reset indexes\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # convert \"yes\" and \"no\" labels to 0/1\n",
    "    df[\"literature_review\"].replace(to_replace=\"yes\", value=1, inplace=True)\n",
    "    df[\"literature_review\"].replace(to_replace=\"no\", value=0, inplace=True)\n",
    "    df.astype({\"literature_review\": int})\n",
    "\n",
    "    # add references column by accessing the opencitations api\n",
    "\n",
    "    api_url = \"https://opencitations.net/index/coci/api/v1/references/\"\n",
    "\n",
    "    new_column = []\n",
    "\n",
    "    for index, row in df.loc[:, [\"doi\"]].iterrows():\n",
    "        \n",
    "        references = []\n",
    "        \n",
    "        if not pd.isna(row.doi):\n",
    "            url = f\"{api_url}{row.doi}\"\n",
    "\n",
    "            # headers = {\"authorization\": \"YOUR-OPENCITATIONS-ACCESS-TOKEN\"}\n",
    "            headers: typing.Dict[str, str] = {}\n",
    "            ret = requests.get(url, headers=headers, timeout=300)\n",
    "            try:\n",
    "                items = json.loads(ret.text)\n",
    "                for item in items:\n",
    "                    references.append(item[\"cited\"])\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                print(f\"Error retrieving citations from OpenCitations for DOI: {row.doi}\")\n",
    "            \n",
    "        if len(references) == 0:\n",
    "            new_column.append(None)\n",
    "        else:\n",
    "            new_column.append(references)\n",
    "\n",
    "    df.insert(loc=len(df.columns), column=\"references\", value=new_column)\n",
    "\n",
    "    # drop doi column\n",
    "    df.drop(['doi'], axis = 1, inplace = True)\n",
    "\n",
    "    # save resulting dataframe to csv file\n",
    "    df.to_csv(store_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make original dataset\n",
    "make_dataset('../../data/external/records_original.bib','../../data/interim/data_original.csv')\n",
    "\n",
    "# make extended dataset\n",
    "make_dataset('../../data/external/records_extended.bib','../../data/interim/data_extended.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
