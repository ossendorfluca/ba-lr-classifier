{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(source_file):\n",
    "    df = pd.read_csv(source_file)\n",
    "    X = df.drop(['literature_review'], axis=1)\n",
    "    y = df['literature_review']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.25, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning with cross validation, smote and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params(X_train,y_train,pipeline,params):\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, scoring=['f1','recall','precision'],cv=5, refit='f1')\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    return grid_search.best_params_, grid_search.score(X_train, y_train)\n",
    "\n",
    "def cv(source_path):\n",
    "    X_train, X_test, y_train, y_test = split(source_path)\n",
    "\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_pipeline = imbpipeline([('sampling', SMOTE()),('scaler', StandardScaler()),('lr', LogisticRegression(max_iter=200))])\n",
    "    lr_params = {'lr__penalty':['l1','l2'], \n",
    "                'lr__C':[1, 10, 100, 1000],\n",
    "                'lr__class_weight': [None, 'balanced']}\n",
    "    lr_best_params, lr_score = tune_params(X_train, y_train, lr_pipeline, lr_params)\n",
    "    print(f'Logistic Regression:\\n best params: {lr_best_params}\\n scores: {lr_score}')\n",
    "    \n",
    "    # Support Vector Machines\n",
    "    svm_pipeline = imbpipeline([('sampling', SMOTE()),('scaler', StandardScaler()),('svm', SVC())])\n",
    "    svm_params = {'svm__C': [0.1, 1, 10],  \n",
    "                'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                'svm__kernel': ['rbf'],\n",
    "                'svm__class_weight': [None, 'balanced']}\n",
    "    svm_best_params, svm_score = tune_params(X_train, y_train, svm_pipeline, svm_params)\n",
    "    print(f'Support Vector Machines:\\n best params: {svm_best_params}\\n scores: {svm_score}')\n",
    "\n",
    "    # Naive Bayes without parameter optimization\n",
    "    pipeline = imbpipeline(steps = [('sampling', SMOTE()), ('nb', GaussianNB())])\n",
    "    stratified_kfold = StratifiedKFold(n_splits=5)\n",
    "    f1_nb = np.mean(cross_val_score(pipeline, X_train, y_train, scoring = 'f1', cv=stratified_kfold))\n",
    "    print(f\"Naive Bayes: \\n f1: {f1_nb}\")\n",
    "\n",
    "    # Decision Trees\n",
    "    dt_pipeline = imbpipeline([('sampling', SMOTE()),('dt', DecisionTreeClassifier())])\n",
    "    dt_params = {'dt__criterion': ['gini', 'entropy'], \n",
    "                'dt__max_depth':range(1,10),\n",
    "                'dt__class_weight': [None, 'balanced']}\n",
    "    dt_best_params, dt_score = tune_params(X_train, y_train, dt_pipeline, dt_params)\n",
    "    print(f'Decision Trees:\\n best params: {dt_best_params}\\n scores: {dt_score}')\n",
    "\n",
    "    # Random Forest\n",
    "    rf_pipeline = imbpipeline([('sampling', SMOTE()),('rf', RandomForestClassifier())])\n",
    "    rf_params = {'rf__bootstrap': [True, False],\n",
    "                 'rf__max_depth': [3, 6, 9, None],\n",
    "                 'rf__max_features': ['auto', 'sqrt'],\n",
    "                 'rf__n_estimators': [25, 50, 100, 150],\n",
    "                 'rf__class_weight': [None, 'balanced']}\n",
    "    rf_best_params, rf_score = tune_params(X_train, y_train, rf_pipeline, rf_params)\n",
    "    print(f'Random Forest:\\n best params: {rf_best_params}\\n scores: {rf_score}')\n",
    "\n",
    "    # k-nearest neighbor\n",
    "    knn_pipeline = imbpipeline([('sampling', SMOTE()),('scaler', StandardScaler()),('knn', KNeighborsClassifier())])\n",
    "    knn_params = {'knn__n_neighbors': range(1,10),  \n",
    "                'knn__weights': ['uniform', 'distance']}\n",
    "    knn_best_params, knn_score = tune_params(X_train, y_train, knn_pipeline, knn_params)\n",
    "    print(f'K-nearest neighbor:\\n best params: {knn_best_params}\\n scores: {knn_score}')\n",
    "    \n",
    "    # Balanced Random Forest\n",
    "    brf_pipeline = imbpipeline([('sampling', SMOTE()),('brf', BalancedRandomForestClassifier())])\n",
    "    brf_params = {'brf__bootstrap': [True, False],\n",
    "                 'brf__max_depth': [3, 6, 9, None],\n",
    "                 'brf__max_features': ['auto', 'sqrt'],\n",
    "                 'brf__n_estimators': [25, 50, 100, 150],\n",
    "                 'brf__class_weight': [None, 'balanced']}\n",
    "    brf_best_params, brf_score = tune_params(X_train, y_train, brf_pipeline, brf_params)\n",
    "    print(f'Balanced Random Forest: \\n best params: {brf_best_params}\\n scores: {brf_score}')\n",
    "\n",
    "    return y_test, X_test, y_train, X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- - - original dataset, keywords feature - - -\")\n",
    "y_test1, X_test1, y_train1, X_train1 = cv(\"../../data/processed/original_dataset/data_key.csv\")\n",
    "\n",
    "print(\"- - - original dataset, references feature - - -\")\n",
    "y_test2, X_test2, y_train2, X_train2 = cv(\"../../data/processed/original_dataset/data_ref.csv\")\n",
    "\n",
    "print(\"- - - original dataset, text mining feature - - -\")\n",
    "y_test3, X_test3, y_train3, X_train3 = cv(\"../../data/processed/original_dataset/data_tm.csv\")\n",
    "\n",
    "print(\"- - - extended dataset, keywords feature - - -\")\n",
    "y_test4, X_test4, y_train4, X_train4 = cv(\"../../data/processed/extended_dataset/data_key.csv\")\n",
    "\n",
    "print(\"- - - extended dataset, references feature - - -\")\n",
    "y_test5, X_test5, y_train5, X_train5 = cv(\"../../data/processed/extended_dataset/data_ref.csv\")\n",
    "\n",
    "print(\"- - - extended dataset, text mining feature - - -\")\n",
    "y_test6, X_test6, y_train6, X_train6 = cv(\"../../data/processed/extended_dataset/data_tm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(estimator, y_test, X_test, y_train, X_train):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(f\"     f1: {f1}\\n     recall: {recall}\\n      precision: {precision}\")\n",
    "\n",
    "print(\"original dataset, keywords feature\")\n",
    "print(\" LR:\")\n",
    "train_test(LogisticRegression(max_iter=200, C=1, class_weight=None, penalty='l2'), y_test1, X_test1, y_train1, X_train1)\n",
    "print(\" SVM:\")\n",
    "train_test(SVC(C=0.1, class_weight=None, gamma=0.01, kernel='rbf'), y_test1, X_test1, y_train1, X_train1)\n",
    "print(\" NB:\")\n",
    "train_test(BernoulliNB(), y_test1, X_test1, y_train1, X_train1)\n",
    "print(\" DT:\")\n",
    "train_test(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1), y_test1, X_test1, y_train1, X_train1)\n",
    "print(\" RF:\")\n",
    "train_test(RandomForestClassifier(bootstrap=True, class_weight=None, max_depth=3, max_features='sqrt', n_estimators=25), y_test1, X_test1, y_train1, X_train1)\n",
    "print(\" kNN:\")\n",
    "train_test(KNeighborsClassifier(n_neighbors=9, weights='distance'), y_test1, X_test1, y_train1, X_train1)\n",
    "print(\" BRF:\")\n",
    "train_test(BalancedRandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=3, max_features='sqrt', n_estimators=100), y_test1, X_test1, y_train1, X_train1)\n",
    "\n",
    "\n",
    "print(\"original dataset, references feature\")\n",
    "print(\" LR:\")\n",
    "train_test(LogisticRegression(max_iter=200, C=1, class_weight='balanced', penalty='l2'), y_test2, X_test2, y_train2, X_train2)\n",
    "print(\" SVM:\")\n",
    "train_test(SVC(C=0.1, class_weight=None, gamma=0.01, kernel='rbf'), y_test2, X_test2, y_train2, X_train2)\n",
    "print(\" NB:\")\n",
    "train_test(BernoulliNB(), y_test2, X_test2, y_train2, X_train2)\n",
    "print(\" DT:\")\n",
    "train_test(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1), y_test2, X_test2, y_train2, X_train2)\n",
    "print(\" RF:\")\n",
    "train_test(RandomForestClassifier(bootstrap=True, class_weight=None, max_depth=9, max_features='sqrt', n_estimators=25), y_test2, X_test2, y_train2, X_train2)\n",
    "print(\" kNN:\")\n",
    "train_test(KNeighborsClassifier(n_neighbors=9, weights='distance'), y_test2, X_test2, y_train2, X_train2)\n",
    "print(\" BRF:\")\n",
    "train_test(BalancedRandomForestClassifier(bootstrap=True, class_weight=None, max_depth=None, max_features='sqrt', n_estimators=25), y_test2, X_test2, y_train2, X_train2)\n",
    "\n",
    "\n",
    "print(\"original dataset, text mining feature\")\n",
    "print(\" LR:\")\n",
    "train_test(LogisticRegression(max_iter=200, C=1, class_weight=None, penalty='l2'), y_test3, X_test3, y_train3, X_train3)\n",
    "print(\" SVM:\")\n",
    "train_test(SVC(C=10, class_weight=None, gamma=0.0001, kernel='rbf'), y_test3, X_test3, y_train3, X_train3)\n",
    "print(\" NB:\")\n",
    "train_test(BernoulliNB(), y_test3, X_test3, y_train3, X_train3)\n",
    "print(\" DT:\")\n",
    "train_test(DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=5), y_test3, X_test3, y_train3, X_train3)\n",
    "print(\" RF:\")\n",
    "train_test(RandomForestClassifier(bootstrap=True, class_weight='balanced', max_depth=None, max_features='sqrt', n_estimators=100), y_test3, X_test3, y_train3, X_train3)\n",
    "print(\" kNN:\")\n",
    "train_test(KNeighborsClassifier(n_neighbors=3, weights='distance'), y_test3, X_test3, y_train3, X_train3)\n",
    "print(\" BRF:\")\n",
    "train_test(BalancedRandomForestClassifier(bootstrap=False, class_weight=None, max_depth=None, max_features='sqrt', n_estimators=25), y_test3, X_test3, y_train3, X_train3)\n",
    "\n",
    "\n",
    "print(\"extended dataset, keywords feature\")\n",
    "print(\" LR:\")\n",
    "train_test(LogisticRegression(max_iter=200, C=1000, class_weight='balanced', penalty='l2'), y_test4, X_test4, y_train4, X_train4)\n",
    "print(\" SVM:\")\n",
    "train_test(SVC(C=0.1, class_weight=None, gamma=0.01, kernel='rbf'), y_test4, X_test4, y_train4, X_train4)\n",
    "print(\" NB:\")\n",
    "train_test(BernoulliNB(), y_test4, X_test4, y_train4, X_train4)\n",
    "print(\" DT:\")\n",
    "train_test(DecisionTreeClassifier(class_weight='balanced', criterion='entropy', max_depth=8), y_test4, X_test4, y_train4, X_train4)\n",
    "print(\" RF:\")\n",
    "train_test(RandomForestClassifier(bootstrap=False, class_weight=None, max_depth=6, max_features='sqrt', n_estimators=50), y_test4, X_test4, y_train4, X_train4)\n",
    "print(\" kNN:\")\n",
    "train_test(KNeighborsClassifier(n_neighbors=9, weights='distance'), y_test4, X_test4, y_train4, X_train4)\n",
    "print(\" BRF:\")\n",
    "train_test(BalancedRandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=6, max_features='sqrt', n_estimators=50), y_test4, X_test4, y_train4, X_train4)\n",
    "\n",
    "\n",
    "print(\"extended dataset, references feature\")\n",
    "print(\" LR:\")\n",
    "train_test(LogisticRegression(max_iter=200, C=100, class_weight='balanced', penalty='l2'), y_test5, X_test5, y_train5, X_train5)\n",
    "print(\" SVM:\")\n",
    "train_test(SVC(C=1, class_weight='balanced', gamma=0.001, kernel='rbf'), y_test5, X_test5, y_train5, X_train5)\n",
    "print(\" NB:\")\n",
    "train_test(BernoulliNB(), y_test5, X_test5, y_train5, X_train5)\n",
    "print(\" DT:\")\n",
    "train_test(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3), y_test5, X_test5, y_train5, X_train5)\n",
    "print(\" RF:\")\n",
    "train_test(RandomForestClassifier(bootstrap=False, class_weight=None, max_depth=6, max_features='sqrt', n_estimators=25), y_test5, X_test5, y_train5, X_train5)\n",
    "print(\" kNN:\")\n",
    "train_test(KNeighborsClassifier(n_neighbors=9, weights='distance'), y_test5, X_test5, y_train5, X_train5)\n",
    "print(\" BRF:\")\n",
    "train_test(BalancedRandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=6, max_features='sqrt', n_estimators=100), y_test5, X_test5, y_train5, X_train5)\n",
    "\n",
    "\n",
    "print(\"extended dataset, text mining feature\")\n",
    "print(\" LR:\")\n",
    "train_test(LogisticRegression(max_iter=200, C=1, class_weight=None, penalty='l2'), y_test6, X_test6, y_train6, X_train6)\n",
    "print(\" SVM:\")\n",
    "train_test(SVC(C=0.1, class_weight=None, gamma=0.001, kernel='rbf'), y_test6, X_test6, y_train6, X_train6)\n",
    "print(\" NB:\")\n",
    "train_test(BernoulliNB(), y_test6, X_test6, y_train6, X_train6)\n",
    "print(\" DT:\")\n",
    "train_test(DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=4), y_test6, X_test6, y_train6, X_train6)\n",
    "print(\" RF:\")\n",
    "train_test(RandomForestClassifier(bootstrap=False, class_weight=None, max_depth=None, max_features='sqrt', n_estimators=50), y_test6, X_test6, y_train6, X_train6)\n",
    "print(\" kNN:\")\n",
    "train_test(KNeighborsClassifier(n_neighbors=5, weights='distance'), y_test6, X_test6, y_train6, X_train6)\n",
    "print(\" BRF:\")\n",
    "train_test(BalancedRandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=None, max_features='sqrt', n_estimators=150), y_test6, X_test6, y_train6, X_train6)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
