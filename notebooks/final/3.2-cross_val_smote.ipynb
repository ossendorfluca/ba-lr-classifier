{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(source_file):\n",
    "    df = pd.read_csv(source_file)\n",
    "    X = df.drop(['literature_review'], axis=1)\n",
    "    y = df['literature_review']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.25, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning with cross validation, smote and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params(X_train,y_train,pipeline,params):\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, scoring=['f1','recall','precision'],cv=5, refit='f1')\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    return grid_search.best_params_, grid_search.score(X_train, y_train)\n",
    "\n",
    "def cv(source_path):\n",
    "    X_train, X_test, y_train, y_test = split(source_path)\n",
    "\n",
    "    '''\n",
    "    # Balanced Random Forest\n",
    "    brf_pipeline = imbpipeline([('brf', BalancedRandomForestClassifier())])\n",
    "    brf_params = {'brf__bootstrap': [True, False],\n",
    "                'brf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                'brf__max_features': ['auto', 'sqrt'],\n",
    "                'brf__min_samples_leaf': [1, 2, 4],\n",
    "                'brf__min_samples_split': [2, 5, 10],\n",
    "                'brf__n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "                'brf__class_weight': [None, 'balanced']}\n",
    "    brf_best_params, brf_score = tune_params(X_train, y_train, brf_pipeline, brf_params)\n",
    "    print(f'Balanced Random Forest: \\n best params: {brf_best_params}\\n scores: {brf_score}')\n",
    "    '''\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_pipeline = imbpipeline([('sampling', SMOTE()),('scaler', StandardScaler()),('lr', LogisticRegression(max_iter=200))])\n",
    "    lr_params = {'lr__penalty':['l1','l2'], \n",
    "                'lr__C':[1, 10, 100, 1000],\n",
    "                'lr__class_weight': [None, 'balanced']}\n",
    "    lr_best_params, lr_score = tune_params(X_train, y_train, lr_pipeline, lr_params)\n",
    "    print(f'Logistic Regression:\\n best params: {lr_best_params}\\n scores: {lr_score}')\n",
    "    \n",
    "    # Support Vector Machines\n",
    "    svm_pipeline = imbpipeline([('sampling', SMOTE()),('scaler', StandardScaler()),('svm', SVC())])\n",
    "    svm_params = {'svm__C': [0.1, 1, 10, 100, 1000],  \n",
    "                'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                'svm__kernel': ['rbf'],\n",
    "                'svm__class_weight': [None, 'balanced']}\n",
    "    svm_best_params, svm_score = tune_params(X_train, y_train, svm_pipeline, svm_params)\n",
    "    print(f'Support Vector Machines:\\n best params: {svm_best_params}\\n scores: {svm_score}')\n",
    "\n",
    "    # Naive Bayes\n",
    "    f1_nb = np.mean(cross_val_score(BernoulliNB(), X_train, y_train, scoring=\"f1\"))\n",
    "    print(f\"Naive Bayes: \\n f1: {f1_nb} -> no parameter optimization!\")\n",
    "\n",
    "    # Decision Trees\n",
    "    dt_pipeline = imbpipeline([('sampling', SMOTE()),('dt', DecisionTreeClassifier())])\n",
    "    dt_params = {'dt__criterion': ['gini', 'entropy'], \n",
    "                'dt__max_depth':range(1,10),\n",
    "                'dt__class_weight': [None, 'balanced']}\n",
    "    dt_best_params, dt_score = tune_params(X_train, y_train, dt_pipeline, dt_params)\n",
    "    print(f'Decision Trees:\\n best params: {dt_best_params}\\n scores: {dt_score}')\n",
    "\n",
    "    # Random Forest\n",
    "    rf_pipeline = imbpipeline([('sampling', SMOTE()),('rf', RandomForestClassifier())])\n",
    "    rf_params = {'rf__bootstrap': [True, False],\n",
    "                    'rf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                    'rf__max_features': ['auto', 'sqrt'],\n",
    "                    'rf__min_samples_leaf': [1, 2, 4],\n",
    "                    'rf__min_samples_split': [2, 5, 10],\n",
    "                    'rf__n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "                    'rf__class_weight': [None, 'balanced']}\n",
    "    rf_best_params, rf_score = tune_params(X_train, y_train, rf_pipeline, rf_params)\n",
    "    print(f'Random Forest:\\n best params: {rf_best_params}\\n scores: {rf_score}')\n",
    "\n",
    "    # k-nearest neighbor\n",
    "    knn_pipeline = imbpipeline([('sampling', SMOTE()),('scaler', StandardScaler()),('knn', KNeighborsClassifier())])\n",
    "    knn_params = {'knn__n_neighbors': range(1,10),  \n",
    "                'knn__weights': ['uniform', 'distance']}\n",
    "    knn_best_params, knn_score = tune_params(X_train, y_train, knn_pipeline, knn_params)\n",
    "    print(f'K-nearest neighbor:\\n best params: {knn_best_params}\\n scores: {knn_score}')\n",
    "\n",
    "    return y_test, X_test, y_train, X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- - - original dataset, keywords feature - - -\")\n",
    "y_test1, X_test1, y_train1, X_train1 = cv(\"../../data/processed/original_dataset/data_key.csv\")\n",
    "\n",
    "print(\"- - - original dataset, references feature - - -\")\n",
    "y_test2, X_test2, y_train2, X_train2 = cv(\"../../data/processed/original_dataset/data_ref.csv\")\n",
    "\n",
    "print(\"- - - original dataset, text mining feature - - -\")\n",
    "y_test3, X_test3, y_train3, X_train3 = cv(\"../../data/processed/original_dataset/data_tm.csv\")\n",
    "\n",
    "print(\"- - - extended dataset, keywords feature - - -\")\n",
    "y_test4, X_test4, y_train4, X_train4 = cv(\"../../data/processed/extended_dataset/data_key.csv\")\n",
    "\n",
    "print(\"- - - extended dataset, references feature - - -\")\n",
    "y_test5, X_test5, y_train5, X_train5 = cv(\"../../data/processed/extended_dataset/data_ref.csv\")\n",
    "\n",
    "print(\"- - - extended dataset, text mining feature - - -\")\n",
    "y_test6, X_test6, y_train6, X_train6 = cv(\"../../data/processed/extended_dataset/data_tm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(estimator, y_test, X_test, y_train, X_train):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    return f1, recall, precision\n",
    "\n",
    "print(\"original dataset, keywords feature\")\n",
    "print(\" LR:\")\n",
    "f1, recall, precision = train_test(LogisticRegression(max_iter=200, C=100, class_weight='balanced', penalty='l2'), y_test1, X_test1, y_train1, X_train1)\n",
    "print(f\"     f1: {f1}\\n     recall: {recall}\\n      precision: {precision}\")\n",
    "print(\" SVM:\")\n",
    "f1, recall, precision = train_test(SVC(C=0.1, class_weight='balanced', gamma=0.001, kernel='rbf'), y_test1, X_test1, y_train1, X_train1)\n",
    "print(f\"     f1: {f1}\\n     recall: {recall}\\n      precision: {precision}\")\n",
    "print(\" NB:\")\n",
    "f1, recall, precision = train_test(BernoulliNB(), y_test1, X_test1, y_train1, X_train1)\n",
    "print(f\"     f1: {f1}\\n     recall: {recall}\\n      precision: {precision}\")\n",
    "print(\" DT:\")\n",
    "f1, recall, precision = train_test(DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=2), y_test1, X_test1, y_train1, X_train1)\n",
    "print(f\"     f1: {f1}\\n     recall: {recall}\\n      precision: {precision}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
